---
title: 使用tidymodels预测二分类变量
author: liyue
date: '2021-08-08'
slug: []
categories:
  - R
tags:
  - tidymodels
lastmod: '2021-08-08T21:23:31+08:00'
keywords: []
description: ''
comment: no
toc: yes
autoCollapseToc: no
postMetaInFooter: no
hiddenFromHomePage: no
contentCopyright: no
reward: no
mathjax: no
mathjaxEnableSingleDollar: no
mathjaxEnableAutoNumber: no
hideHeaderAndFooter: no
flowchartDiagrams:
  enable: no
  options: ''
sequenceDiagrams:
  enable: no
  options: ''
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,collapse = TRUE,warning = FALSE)
```

上次尝试了`tidymodels`在预测连续型变量中的应用，本次分享的是一个结果为二分类变量的例子。
数据依然是来自于[tidytuesday](https://github.com/rfordatascience/tidytuesday)，可以根据日期自己下载使用。

本次数据是根据一些列变量来预测住旅馆的人会不会带孩子的数据，通过不同的变量，比如入住时间、房型、住几晚、之前的入住信息等，来预测住宾馆的人到底有没有带小孩。

## 数据探索

children是结果变量，是一个2分类变量

```{r}
library(tidyverse)

hotels <- readr::read_csv("E:/git/machine-learning-r-package/datasets/tidytuesday/data/2020/2020-02-11/hotels.csv")


hotel_stays <- hotels %>%
  filter(is_canceled == 0) %>%
  mutate(
    children = case_when( # 学习下case_when函数
      children + babies > 0 ~ "children",
      TRUE ~ "none"
    ),
    required_car_parking_spaces = case_when(
      required_car_parking_spaces > 0 ~ "parking",
      TRUE ~ "none"
    )
  ) %>%
  select(-is_canceled, -reservation_status, -babies)

hotel_stays
```

可以看到一共有75166行，29列，为了节省运算时间（电脑配置不够，:cry），只选取了部分数据。

看一下结果变量的分布情况

```{r}
hotel_stays %>% count(children) # children是结果变量
```

简单看下数据具体情况：

```{r}
skimr::skim(hotel_stays)
```

画个图看一下

```{r}
# 写出这么刘畅的代码取决于对数据的理解以及对tidyverse包的理解
hotel_stays %>%
  mutate(arrival_date_month = factor(arrival_date_month, levels = month.name)) %>%
  count(hotel, arrival_date_month, children) %>%
  group_by(hotel, children) %>%
  mutate(proportion = n / sum(n)) %>%
  ggplot(aes(arrival_date_month, proportion, fill = children)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = scales::percent_format()) + # 学习下百分比的用法
  facet_wrap(~hotel, nrow = 2) +
  labs(x = NULL, y = "Proportion of hotel stays", fill = NULL)
```

再画个图看一下：

```{r}
hotel_stays %>%
  count(hotel, required_car_parking_spaces, children) %>%
  group_by(hotel, children) %>%
  mutate(proportion = n / sum(n)) %>%
  ggplot(aes(required_car_parking_spaces, proportion, fill = children)) +
  geom_col(position = "dodge") +
  scale_y_continuous(labels = scales::percent_format()) +
  facet_wrap(~hotel, nrow = 2) +
  labs(x = NULL, y = "Proportion of hotel stays", fill = NULL)
```

画一个更复杂的图看一下：

```{r}
library(GGally)

hotel_stays %>%
  select(
    children, adr,
    required_car_parking_spaces,
    total_of_special_requests
  ) %>%
  ggpairs(mapping = aes(color = children))
```


接下来是正是的建模过程了。

## 建模

### 数据准备

选取自己需要的数据：

```{r}
hotels_df <- hotel_stays %>%
  select(
    children, hotel, arrival_date_month, meal, adr, adults,
    required_car_parking_spaces, total_of_special_requests,
    stays_in_week_nights, stays_in_weekend_nights
  ) %>%
  mutate_if(is.character, factor)

hotels_df <- hotels_df[1:1000,]
```

数据预处理过程，主要是通过`recipes`包完成。

```{r}
library(tidymodels)
tidymodels_prefer()

set.seed(12)
hotel_split <- hotels_df %>% 
  initial_split()

hotel_train <- training(hotel_split)
hotel_test <- testing(hotel_split)

hotel_rec <- recipe(children ~ ., data = hotel_train) %>%
  themis::step_downsample(children) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_numeric()) %>%
  step_normalize(all_numeric()) %>%
  prep()

hotel_rec
```

分割数据：

```{r}
# 提取准备好的训练集和测试集
train_proc <- bake(hotel_rec, new_data = NULL)
test_proc <- bake(hotel_rec, new_data = hotel_test)
```

### knn

```{r}
knn_spec <- nearest_neighbor(mode = "classification") %>% set_engine("kknn")
knn_fit <- knn_spec %>% fit(children ~ ., data = train_proc)

knn_fit
```

### 随机森林

```{r}
rf_spec <- rand_forest(mode = "classification") %>% set_engine("ranger")
rf_fit <- rf_spec %>% fit(children ~ ., data = train_proc)

rf_fit
```

### 决策树

```{r}
tree_spec <- decision_tree(mode = "classification") %>% set_engine("rpart")
tree_fit <- tree_spec %>% fit(children ~ ., data = train_proc)

tree_fit
```

## 评价模型

```{r}
set.seed(1234)

# 模特卡洛法交叉验证
validation_split <- mc_cv(data = train_proc, prop = 0.9, strata = children)
validation_split

validation_split$splits[[1]] %>% analysis() %>% dim()
validation_split$splits[[1]] %>% assessment()
```

knn的交叉验证

```{r}
knn_res <- fit_resamples(
  knn_spec,
  children ~ ., 
  validation_split,
  control = control_resamples(save_pred = T)
)

knn_res
```

查看结果

```{r}
knn_res %>% collect_metrics()
knn_res %>% collect_metrics(summarise=FALSE)
```

查看每一折里面的结果

```{r}
knn_res %>% collect_predictions()
```

随机森林的结果

```{r}
rf_res <- fit_resamples(
  rf_spec,
  children ~ ., 
  validation_split,
  control = control_resamples(save_pred = T)
)

rf_res %>% collect_metrics()
```

决策树结果

```{r}
tree_res <- fit_resamples(
  tree_spec,
  children ~ ., 
  validation_split,
  control = control_resamples(save_pred = T)
)

tree_res %>% collect_metrics()
```

3种算法放在一起画图：

```{r}
knn_res %>%
  unnest(.predictions) %>%
  mutate(model = "kknn") %>%
  bind_rows(tree_res %>%
    unnest(.predictions) %>%
    mutate(model = "rpart")) %>%
  bind_rows(rf_res %>% 
    unnest(.predictions) %>% 
    mutate(model = "ranger")) %>% 
  group_by(model) %>%
  roc_curve(children, .pred_children) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line(size = 1.5) +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```

从上图中可以看出随机森林最好

下面是一个混淆矩阵的结果：

```{r}
knn_conf <- knn_res %>% unnest(.predictions) %>% 
  conf_mat(children, .pred_class)

knn_conf
```

可视化混淆矩阵：

```{r}
knn_conf %>% autoplot()
```

使用随机森林预测新的数据：

```{r}
rf_fit %>% predict(new_data = test_proc, type = "prob") %>% 
  mutate(truth = hotel_test$children) %>% 
  roc_auc(truth, .pred_children)
```
